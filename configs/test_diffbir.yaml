data:
  target: dataset.data_module.BIRDataModule
  params:
    # Path to training set configuration file.
    train_config: /home/user001/zwl/zyx/Diffbir/configs/dataset/face_train.yaml
    # Path to validation set configuration file.
    val_config: /home/user001/zwl/zyx/Diffbir/configs/dataset/face_test_lq.yaml

model:
  # You can set learning rate in the following configuration file.
  config: configs/model/diffbir_eval.yaml
  # Path to the checkpoints or weights you want to resume. At the begining, 
  # this should be set to the initial weights created by scripts/make_stage2_init_weight.py.
  #resume:  /home/zyx/DiffBIR-main/weights/general-step=79999.ckpt
  resume: /home/user001/zwl/data/cldm_weights/face_full_v1.ckpt
  #resume: /home/zyx/DiffBIR-main/work_dirs/exp24_general_all/lightning_logs/version_2/checkpoints/step=64999.ckpt
  # resume: /home/zyx/DiffBIR-main/work_dirs/exp22_r=8_lr=1-4/lightning_logs/version_2/checkpoints/step=244999.ckpt
 
  # /home/zyx/DiffBIR-main/work_dirs/exp22_r=8_lr=1-4/lightning_logs/version_2/checkpoints/step=24999.ckpt
  # /home/zyx/DiffBIR-main/work_dirs/exp21_r=8_lr=1-5/lightning_logs/version_0/checkpoints/step=19999.ckpt 16.3
  # resume: /home/zyx/DiffBIR-main/work_dirs/exp17/lightning_logs/version_0/checkpoints/step=189999.ckpt
  #/home/zyx/DiffBIR-main/work_dirs/exp22_r=8_lr=1-4/lightning_logs/version_2/checkpoints/step=54999.ckpt #33.98
  #/home/zyx/DiffBIR-main/work_dirs/exp17/lightning_logs/version_0/checkpoints/step=119999.ckpt #15.9
  
  #/home/zyx/DiffBIR-main/work_dirs/exp21_r=8_lr=1-5/lightning_logs/version_0/checkpoints/step=4999.ckpt
lightning:
  seed: 231
  
  trainer:
    accelerator: ddp
    precision: 32
    # Indices of GPUs used for training.
    gpus: [0]
    # Path to save logs and checkpoints.
    default_root_dir: ./work_dirs/testcelebamaskhq_1
    # Max number of training steps (batches).
    max_steps: 250001
    # Validation frequency in terms of training steps.
    val_check_interval: 500
    log_every_n_steps: 50
    # Accumulate gradients from multiple batches so as to increase batch size.
    accumulate_grad_batches: 1
  
  callbacks:
    - target: model.callbacks.ImageLogger
      params:
        # Log frequency of image logger.
        log_every_n_steps: 250
        max_images_each_step: 4
        log_images_kwargs: ~

    - target: model.callbacks.ModelCheckpoint
      params:
        # Frequency of saving checkpoints.
        every_n_train_steps: 5000
        save_top_k: -1
        filename: "{step}"
